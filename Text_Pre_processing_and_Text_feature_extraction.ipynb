{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rouakhadhraoui/Text-Mining-Labs-/blob/main/1_Text_Pre_processing_and_Text_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download required resources (only once)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"punkt_tab\") # Added this line to download the missing resource\n",
        "\n",
        "corpus = \"\"\"Text mining seeks to extract useful information from data sources\n",
        "through the identification and exploration of interesting patterns.\n",
        "The data sources are document collections, and interesting patterns are found\n",
        "not among formalized database records but in the unstructured textual data\n",
        "in the documents in these collections.\"\"\"\n",
        "\n",
        "print(\"Original Corpus:\\n\", corpus)\n",
        "\n",
        "# Step 1: Lowercase all corpus words\n",
        "corpus_lower = corpus.lower()\n",
        "print(\"\\nStep 1 - Lowercase:\\n\", corpus_lower)\n",
        "\n",
        "# Step 2: Split into sentences, then into words\n",
        "sentences = sent_tokenize(corpus_lower)   # sentence tokenization\n",
        "print(\"\\nStep 2 - Tokenized sentences:\\n\", sentences)\n",
        "\n",
        "words = [word_tokenize(sent) for sent in sentences]  # word tokenization\n",
        "print(\"\\nStep 2 - Tokenized words:\\n\", words)\n",
        "\n",
        "# Step 3: Remove punctuation\n",
        "words_no_punc = [[w for w in sent if w not in string.punctuation] for sent in words]\n",
        "print(\"\\nStep 3 - Without punctuation:\\n\", words_no_punc)\n",
        "\n",
        "# Step 4: Remove Stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words_no_stop = [[w for w in sent if w not in stop_words] for sent in words_no_punc]\n",
        "print(\"\\nStep 4 - Without stopwords:\\n\", words_no_stop)\n",
        "\n",
        "# Step 5: Stemming vs Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming and lemmatization\n",
        "stemming = [[stemmer.stem(w) for w in sent] for sent in words_no_stop]\n",
        "lemmatization = [[lemmatizer.lemmatize(w) for w in sent] for sent in words_no_stop]\n",
        "\n",
        "print(\"\\nStep 5 - Stemming:\\n\", stemming)\n",
        "print(\"\\nStep 5 - Lemmatization:\\n\", lemmatization)"
      ],
      "metadata": {
        "id": "sqnsMzBY0xq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b5dbd9-36ae-4ef1-c10a-9a854f9ec4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Corpus:\n",
            " Text mining seeks to extract useful information from data sources\n",
            "through the identification and exploration of interesting patterns.\n",
            "The data sources are document collections, and interesting patterns are found\n",
            "not among formalized database records but in the unstructured textual data\n",
            "in the documents in these collections.\n",
            "\n",
            "Step 1 - Lowercase:\n",
            " text mining seeks to extract useful information from data sources\n",
            "through the identification and exploration of interesting patterns.\n",
            "the data sources are document collections, and interesting patterns are found\n",
            "not among formalized database records but in the unstructured textual data\n",
            "in the documents in these collections.\n",
            "\n",
            "Step 2 - Tokenized sentences:\n",
            " ['text mining seeks to extract useful information from data sources\\nthrough the identification and exploration of interesting patterns.', 'the data sources are document collections, and interesting patterns are found\\nnot among formalized database records but in the unstructured textual data\\nin the documents in these collections.']\n",
            "\n",
            "Step 2 - Tokenized words:\n",
            " [['text', 'mining', 'seeks', 'to', 'extract', 'useful', 'information', 'from', 'data', 'sources', 'through', 'the', 'identification', 'and', 'exploration', 'of', 'interesting', 'patterns', '.'], ['the', 'data', 'sources', 'are', 'document', 'collections', ',', 'and', 'interesting', 'patterns', 'are', 'found', 'not', 'among', 'formalized', 'database', 'records', 'but', 'in', 'the', 'unstructured', 'textual', 'data', 'in', 'the', 'documents', 'in', 'these', 'collections', '.']]\n",
            "\n",
            "Step 3 - Without punctuation:\n",
            " [['text', 'mining', 'seeks', 'to', 'extract', 'useful', 'information', 'from', 'data', 'sources', 'through', 'the', 'identification', 'and', 'exploration', 'of', 'interesting', 'patterns'], ['the', 'data', 'sources', 'are', 'document', 'collections', 'and', 'interesting', 'patterns', 'are', 'found', 'not', 'among', 'formalized', 'database', 'records', 'but', 'in', 'the', 'unstructured', 'textual', 'data', 'in', 'the', 'documents', 'in', 'these', 'collections']]\n",
            "\n",
            "Step 4 - Without stopwords:\n",
            " [['text', 'mining', 'seeks', 'extract', 'useful', 'information', 'data', 'sources', 'identification', 'exploration', 'interesting', 'patterns'], ['data', 'sources', 'document', 'collections', 'interesting', 'patterns', 'found', 'among', 'formalized', 'database', 'records', 'unstructured', 'textual', 'data', 'documents', 'collections']]\n",
            "\n",
            "Step 5 - Stemming:\n",
            " [['text', 'mine', 'seek', 'extract', 'use', 'inform', 'data', 'sourc', 'identif', 'explor', 'interest', 'pattern'], ['data', 'sourc', 'document', 'collect', 'interest', 'pattern', 'found', 'among', 'formal', 'databas', 'record', 'unstructur', 'textual', 'data', 'document', 'collect']]\n",
            "\n",
            "Step 5 - Lemmatization:\n",
            " [['text', 'mining', 'seek', 'extract', 'useful', 'information', 'data', 'source', 'identification', 'exploration', 'interesting', 'pattern'], ['data', 'source', 'document', 'collection', 'interesting', 'pattern', 'found', 'among', 'formalized', 'database', 'record', 'unstructured', 'textual', 'data', 'document', 'collection']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "\n",
        "corpus = \"\"\"Text mining seeks to extract useful information from data sources\n",
        "through the identification and exploration of interesting patterns.\n",
        "The data sources are document collections, and interesting patterns are found\n",
        "not among formalized database records but in the unstructured textual data\n",
        "in the documents in these collections.\"\"\"\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit([corpus]) # Put the corpus in a list\n",
        "# encode document\n",
        "newvector = vectorizer.transform([corpus]) # Put the corpus in a list\n",
        "# summarize encoded vector\n",
        "print(newvector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF6KuOQZ8yY2",
        "outputId": "b47b0a8e-48fe-4ed2-b24f-bcdea08b231d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 2 1 2 3 1 1 1 1 1 1 1 1 1 3 1 2 1 1 1 2 1 1 2 1 1 4 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = \"\"\"Text mining seeks to extract useful information from data sources\n",
        "through the identification and exploration of interesting patterns.\n",
        "The data sources are document collections, and interesting patterns are found\n",
        "not among formalized database records but in the unstructured textual data\n",
        "in the documents in these collections.\"\"\"\n",
        "\n",
        "# Créer le vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Construire le vocabulaire et transformer le texte en vecteur\n",
        "vector = vectorizer.fit_transform([corpus])\n",
        "\n",
        "# Obtenir le vocabulaire (mots uniques)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Obtenir le nombre d'occurrences\n",
        "counts = vector.toarray().flatten()  # transforme en 1D\n",
        "\n",
        "# Associer chaque mot à son nombre d'occurrences\n",
        "word_counts = dict(zip(words, counts))\n",
        "\n",
        "# Afficher le résultat\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYVbMwvLJox2",
        "outputId": "ea3f251a-94f2-4fab-9d51-593de485ebb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "among: 1\n",
            "and: 2\n",
            "are: 2\n",
            "but: 1\n",
            "collections: 2\n",
            "data: 3\n",
            "database: 1\n",
            "document: 1\n",
            "documents: 1\n",
            "exploration: 1\n",
            "extract: 1\n",
            "formalized: 1\n",
            "found: 1\n",
            "from: 1\n",
            "identification: 1\n",
            "in: 3\n",
            "information: 1\n",
            "interesting: 2\n",
            "mining: 1\n",
            "not: 1\n",
            "of: 1\n",
            "patterns: 2\n",
            "records: 1\n",
            "seeks: 1\n",
            "sources: 2\n",
            "text: 1\n",
            "textual: 1\n",
            "the: 4\n",
            "these: 1\n",
            "through: 1\n",
            "to: 1\n",
            "unstructured: 1\n",
            "useful: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "\n",
        "corpus = \"\"\"Text mining seeks to extract useful information from data sources\n",
        "through the identification and exploration of interesting patterns.\n",
        "The data sources are document collections, and interesting patterns are found\n",
        "not among formalized database records but in the unstructured textual data\n",
        "in the documents in these collections.\"\"\"\n",
        "\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit([corpus])\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform([corpus])\n",
        "# summarize encoded vector\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJiOaIen-d-H",
        "outputId": "eed9c897-b8d4-4bf4-fe07-a92c4bf95c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 25, 'mining': 18, 'seeks': 23, 'to': 30, 'extract': 10, 'useful': 32, 'information': 16, 'from': 13, 'data': 5, 'sources': 24, 'through': 29, 'the': 27, 'identification': 14, 'and': 1, 'exploration': 9, 'of': 20, 'interesting': 17, 'patterns': 21, 'are': 2, 'document': 7, 'collections': 4, 'found': 12, 'not': 19, 'among': 0, 'formalized': 11, 'database': 6, 'records': 22, 'but': 3, 'in': 15, 'unstructured': 31, 'textual': 26, 'documents': 8, 'these': 28}\n",
            "[[0.11043153 0.22086305 0.22086305 0.11043153 0.22086305 0.33129458\n",
            "  0.11043153 0.11043153 0.11043153 0.11043153 0.11043153 0.11043153\n",
            "  0.11043153 0.11043153 0.11043153 0.33129458 0.11043153 0.22086305\n",
            "  0.11043153 0.11043153 0.11043153 0.22086305 0.11043153 0.11043153\n",
            "  0.22086305 0.11043153 0.11043153 0.4417261  0.11043153 0.11043153\n",
            "  0.11043153 0.11043153 0.11043153]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fe0c8f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7e29eb-65f0-4582-aed9-0df1907b28e0"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# define tokenized sentences as training data\n",
        "tokenized_sentences = [corpus.split(),\n",
        "corpus.split() ]\n",
        "# training word2vec model\n",
        "model = Word2Vec(tokenized_sentences,min_count=1)\n",
        "# summarizing the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.index_to_key)\n",
        "print(words)\n",
        "# access word vector for one word \"sources\"\n",
        "print(model.wv['sources'])\n",
        "# try finding most similar words for word \"interesting\"\n",
        "print(model.wv.most_similar('interesting'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD-pnF07-wJ0",
        "outputId": "e79de612-4010-48a3-ccb7-d6dd067d6d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=36, vector_size=100, alpha=0.025>\n",
            "['in', 'the', 'data', 'sources', 'are', 'and', 'interesting', 'collections.', 'exploration', 'identification', 'through', 'from', 'information', 'useful', 'extract', 'to', 'seeks', 'mining', 'of', 'patterns.', 'these', 'The', 'document', 'collections,', 'patterns', 'found', 'not', 'among', 'formalized', 'database', 'records', 'but', 'unstructured', 'textual', 'documents', 'Text']\n",
            "[-8.2059642e-03  9.2979902e-03 -1.9116134e-04 -1.9303083e-03\n",
            "  4.5893337e-03 -4.1022827e-03  2.7695557e-03  6.9844709e-03\n",
            "  6.0466342e-03 -7.5571863e-03  9.3764113e-03  4.6535148e-03\n",
            "  3.9848424e-03 -6.2149838e-03  8.4709413e-03 -2.1671467e-03\n",
            "  8.8483533e-03 -5.3912718e-03 -8.1389090e-03  6.7731421e-03\n",
            "  1.6736966e-03 -2.2028650e-03  9.5166238e-03  9.4885398e-03\n",
            " -9.7884899e-03  2.5083374e-03  6.1256420e-03  3.8669235e-03\n",
            "  2.0322327e-03  4.6207427e-04  7.0411951e-04 -3.8652227e-03\n",
            " -7.1346560e-03 -2.1369199e-03  3.8962394e-03  8.8493060e-03\n",
            "  9.2667416e-03 -5.9722196e-03 -9.4188107e-03  9.7279325e-03\n",
            "  3.4183713e-03  5.1464508e-03  6.2882118e-03 -2.7728549e-03\n",
            "  7.3225265e-03  2.8051892e-03  2.8557535e-03 -2.4018313e-03\n",
            " -3.1210904e-03 -2.3476481e-03  4.2673079e-03  3.1898631e-05\n",
            " -9.6062208e-03 -9.6592931e-03 -6.1710179e-03 -9.5603995e-05\n",
            "  2.0100402e-03  9.4230333e-03  5.5762427e-03 -4.2998088e-03\n",
            "  2.7615507e-04  4.9430085e-03  7.7492204e-03 -1.1285102e-03\n",
            "  4.2874105e-03 -5.7520745e-03 -8.1078801e-04  8.1152739e-03\n",
            " -2.4026630e-03 -9.6611837e-03  5.7952674e-03 -3.8950008e-03\n",
            " -1.2023696e-03  9.9840993e-03 -2.2244703e-03 -4.7594970e-03\n",
            " -5.3375899e-03  7.0170662e-03 -5.7272315e-03  2.1057965e-03\n",
            " -5.2792476e-03  6.1034099e-03  4.3438310e-03  2.6077754e-03\n",
            " -1.4802419e-03 -2.7550445e-03  8.9953225e-03  5.1980303e-03\n",
            " -2.1458969e-03 -9.4557879e-03 -7.4024894e-03 -1.0626721e-03\n",
            " -8.0786581e-04 -2.5440366e-03  9.7023407e-03 -4.6851093e-04\n",
            "  5.8445139e-03 -7.4673644e-03 -2.5010207e-03 -5.5336575e-03]\n",
            "[('formalized', 0.3058081567287445), ('not', 0.17941534519195557), ('seeks', 0.16641205549240112), ('found', 0.16376285254955292), ('data', 0.15057413280010223), ('documents', 0.11531046032905579), ('patterns.', 0.11496210098266602), ('The', 0.09796314686536789), ('of', 0.07840549945831299), ('through', 0.0527837760746479)]\n"
          ]
        }
      ]
    }
  ]
}
