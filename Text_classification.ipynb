{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rouakhadhraoui/Text-Mining-Labs-/blob/main/2_Text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUk2xRjTMxpv",
        "outputId": "b7fcfeb8-1dc2-4a97-ef64-aae6cb7f9ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined and cleaned dataset:\n",
            "       id   airline                                              tweet  label\n",
            "0   95181  American           no excuse for lost luggage youhaveonejob      1\n",
            "1   20845  American  i thought airport wifi was ridiculous until i ...      1\n",
            "2   32473    United  i hope you guys go surfing if youre going to l...      0\n",
            "3  165082    United  my flight to la had no electricity for passeng...      1\n",
            "4   37552   JetBlue  poop announces new bag fees and squeezing out ...      1\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3400 entries, 0 to 3399\n",
            "Data columns (total 4 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   id       3400 non-null   int64 \n",
            " 1   airline  3400 non-null   object\n",
            " 2   tweet    3400 non-null   object\n",
            " 3   label    3400 non-null   int64 \n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 106.4+ KB\n",
            "None\n",
            "Accuracy: 0.7161764705882353\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.61      0.68       336\n",
            "           1       0.68      0.82      0.75       344\n",
            "\n",
            "    accuracy                           0.72       680\n",
            "   macro avg       0.73      0.71      0.71       680\n",
            "weighted avg       0.72      0.72      0.71       680\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[205 131]\n",
            " [ 62 282]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Charger les datasets\n",
        "df_complaint = pd.read_csv(\"complaint1700.csv\")\n",
        "df_noncomplaint = pd.read_csv(\"noncomplaint1700.csv\")\n",
        "\n",
        "# Ajouter la colonne label\n",
        "df_complaint['label'] = 1      # 1 = complaint\n",
        "df_noncomplaint['label'] = 0   # 0 = non-complaint\n",
        "\n",
        "# Combiner les datasets\n",
        "df = pd.concat([df_complaint, df_noncomplaint], ignore_index=True)\n",
        "\n",
        "# Nettoyage du texte\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # mettre en minuscules\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # enlever URL\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)  # enlever mentions et hashtags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # enlever ponctuation et chiffres\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # enlever espaces multiples\n",
        "    return text\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Mélanger les données\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Vérifier\n",
        "print(\"Combined and cleaned dataset:\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Séparer features et target\n",
        "X = df['tweet']\n",
        "y = df['label']\n",
        "\n",
        "# Split en train et test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorisation TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Modèle Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Prédiction\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Évaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    }
  ]
}
